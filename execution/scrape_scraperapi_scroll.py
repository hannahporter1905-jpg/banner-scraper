import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import re
import sys
import time

def extract_all_banners(soup, base_url):
    """Extract ALL banner images from the page"""
    banners = []
    seen_urls = set()
    
    print("ğŸ” Searching for banner images...\n")
    
    # Method 1: Find ALL images with Rails storage paths
    rails_images = soup.find_all('img', src=re.compile(r'/cms/rails/active_storage'))
    print(f"ğŸ“¦ Found {len(rails_images)} images with Rails storage paths")
    
    for img in rails_images:
        src = img.get('src', '')
        if src and src not in seen_urls:
            seen_urls.add(src)
            if not src.startswith('http'):
                src = urljoin(base_url, src)
            
            alt = img.get('alt', 'Promotion Banner')
            
            banners.append({
                'src': src,
                'alt': alt,
                'type': 'Promotion Banner',
                'source': 'Rails Storage'
            })
    
    # Method 2: Find ALL promotion cards (even with empty src)
    promo_cards = soup.find_all('div', class_='promotions-card')
    print(f"ğŸ´ Found {len(promo_cards)} promotion card containers")
    
    for card in promo_cards:
        title_elem = card.find('h4', class_='promotions-card__title')
        title = title_elem.text.strip() if title_elem else 'Promotion'
        
        text_elem = card.find('p', class_='promotions-card__text')
        description = text_elem.text.strip() if text_elem else ''
        
        # Record even if no image (for debugging)
        img = card.find('img', class_='promotions-card__background')
        if img:
            src = img.get('src', '')
            if src and src not in seen_urls:
                seen_urls.add(src)
                if not src.startswith('http'):
                    src = urljoin(base_url, src)
                
                banners.append({
                    'src': src,
                    'alt': img.get('alt', title),
                    'title': title,
                    'description': description[:50],
                    'type': 'Promotion Card',
                    'source': 'Card Container'
                })
        else:
            print(f"  âš ï¸  Card found but no image: {title}")
    
    # Method 3: Hero/Top banner
    hero_banner = soup.find('img', class_='casino-promotions__background')
    if hero_banner:
        src = hero_banner.get('src', '')
        if src and src not in seen_urls:
            seen_urls.add(src)
            if not src.startswith('http'):
                src = urljoin(base_url, src)
            
            banners.append({
                'src': src,
                'alt': 'Welcome Package Banner',
                'title': 'Hero Banner',
                'description': 'Top welcome banner',
                'type': 'Hero Banner',
                'source': 'Top Section'
            })
            print(f"âœ… Found hero banner")
    
    # Method 4: Find ANY image with promotional keywords
    all_images = soup.find_all('img')
    promo_keywords = ['deposit', 'promo', 'banner', 'fortune', 'funday', 'spin', 'wheel', 'bonus', 'mob']
    
    for img in all_images:
        src = img.get('src', '')
        alt = img.get('alt', '').lower()
        
        if src and src not in seen_urls:
            # Check if src contains promo keywords
            src_lower = src.lower()
            if any(keyword in alt or keyword in src_lower for keyword in promo_keywords):
                seen_urls.add(src)
                if not src.startswith('http'):
                    src = urljoin(base_url, src)
                
                banners.append({
                    'src': src,
                    'alt': img.get('alt', 'Promotional Image'),
                    'type': 'Promotional Image',
                    'source': 'Keyword Match'
                })
    
    return banners

def scrape_with_scroll(url, api_key, country_code='us'):
    """
    ScraperAPI with extended wait time and JavaScript rendering
    to allow page to fully load including lazy-loaded images
    """
    
    api_url = 'http://api.scraperapi.com'
    
    # Extended parameters for better image loading
    params = {
        'api_key': api_key,
        'url': url,
        'country_code': country_code,
        'render': 'true',           # Enable JavaScript
        'wait_for_selector': 'img', # Wait for images to appear
        'session_number': '123',    # Use same session for consistency
    }
    
    print(f"\n{'='*70}")
    print(f"  ğŸŒ ScraperAPI with Extended Loading")
    print(f"{'='*70}\n")
    print(f"ğŸ¯ Target: {url}")
    print(f"ğŸ“ Country: {country_code.upper()}")
    print(f"â³ Using JavaScript rendering with image waiting...")
    print(f"   (This ensures lazy-loaded images have time to appear)\n")
    
    try:
        # Make request
        print("ğŸš€ Sending request to ScraperAPI...")
        response = requests.get(api_url, params=params, timeout=120)
        response.raise_for_status()
        
        print(f"âœ… Success! Status: {response.status_code}")
        print(f"ğŸ“Š Page size: {len(response.content):,} bytes\n")
        
        # Save debug HTML
        with open('scraperapi_scroll_debug.html', 'w', encoding='utf-8') as f:
            f.write(response.text)
        print(f"ğŸ’¾ Saved HTML to: scraperapi_scroll_debug.html")
        print(f"   (Open this in a browser to see what was captured)\n")
        
        # Parse
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Extract banners
        print(f"{'='*70}")
        print(f"  ğŸ” ANALYZING PAGE CONTENT")
        print(f"{'='*70}\n")
        
        banners = extract_all_banners(soup, url)
        
        print(f"\n{'='*70}")
        print(f"âœ… Extraction complete!")
        print(f"ğŸ“Š Total unique banners found: {len(banners)}")
        print(f"{'='*70}\n")
        
        return banners
        
    except requests.Timeout:
        print("âŒ Error: Request timed out (took longer than 2 minutes)")
        print("   The website might be very slow or blocking the request")
        return []
    except requests.RequestException as e:
        print(f"âŒ Error: {e}")
        return []

def main():
    print("\n" + "=" * 70)
    print("  ğŸ¯ ScraperAPI Banner Scraper (Optimized for All Images)")
    print("=" * 70)
    print("\nâœ¨ Features:")
    print("  â€¢ Uses ScraperAPI to bypass geo-restrictions")
    print("  â€¢ Waits for images to load with JavaScript rendering")
    print("  â€¢ wait_for_selector ensures images appear before scraping")
    print("  â€¢ Multiple detection methods for maximum coverage")
    print("=" * 70 + "\n")
    
    # Get API key
    api_key = input("ğŸ”‘ Enter your ScraperAPI key: ").strip()
    if not api_key:
        print("\nâŒ API key required!")
        print("   Sign up at: https://www.scraperapi.com")
        sys.exit(1)
    
    # Get URL
    url = input("\nğŸŒ Enter website URL: ").strip()
    if not url:
        print("âŒ URL cannot be empty")
        sys.exit(1)
    
    if not url.startswith(('http://', 'https://')):
        url = 'https://' + url
    
    # Get country
    print("\nğŸ“ Countries: us, uk, ca, au, de, fr, jp, sg, br, in")
    country = input("ğŸ—ºï¸  Country code (default: us): ").strip().lower() or 'us'
    
    print("\n" + "=" * 70)
    
    # Scrape
    start_time = time.time()
    banners = scrape_with_scroll(url, api_key, country)
    elapsed = time.time() - start_time
    
    # Results
    print("=" * 70)
    print("  ğŸ“Š FINAL RESULTS")
    print("=" * 70 + "\n")
    
    if banners:
        print(f"âœ… SUCCESS! Found {len(banners)} banner image(s)")
        print(f"â±ï¸  Time taken: {elapsed:.1f} seconds\n")
        
        for i, banner in enumerate(banners, 1):
            print(f"{'â”€'*70}")
            print(f"ğŸ“¸ Banner #{i}")
            print(f"{'â”€'*70}")
            if 'title' in banner:
                print(f"  ğŸ·ï¸  Title: {banner['title']}")
            if 'description' in banner and banner['description']:
                print(f"  ğŸ“ Description: {banner['description']}")
            print(f"  ğŸ’¬ Alt: {banner['alt']}")
            print(f"  ğŸ”– Type: {banner['type']}")
            print(f"  ğŸ“ Source: {banner['source']}")
            print(f"  ğŸ”— URL: {banner['src'][:80]}{'...' if len(banner['src']) > 80 else ''}")
            print()
        
        # Save to file
        print(f"{'='*70}")
        print("ğŸ’¾ Saving results...")
        
        with open('banner_results.txt', 'w', encoding='utf-8') as f:
            f.write("BANNER SCRAPING RESULTS\n")
            f.write("=" * 70 + "\n\n")
            f.write(f"Website: {url}\n")
            f.write(f"Country: {country.upper()}\n")
            f.write(f"Total Banners: {len(banners)}\n")
            f.write(f"Time Taken: {elapsed:.1f} seconds\n\n")
            
            for i, banner in enumerate(banners, 1):
                f.write(f"\nBanner #{i}\n")
                f.write(f"{'â”€'*70}\n")
                if 'title' in banner:
                    f.write(f"Title: {banner['title']}\n")
                if 'description' in banner:
                    f.write(f"Description: {banner['description']}\n")
                f.write(f"Alt: {banner['alt']}\n")
                f.write(f"Type: {banner['type']}\n")
                f.write(f"Source: {banner['source']}\n")
                f.write(f"URL: {banner['src']}\n")
        
        print("âœ… Results saved to: banner_results.txt")
        print("âœ… HTML saved to: scraperapi_scroll_debug.html")
        
    else:
        print("âŒ No banners found")
        print("\nğŸ’¡ Tips:")
        print("  â€¢ Check scraperapi_scroll_debug.html to see what was captured")
        print("  â€¢ Try a different country code")
        print("  â€¢ Some sites may still block even with ScraperAPI")
    
    print("\n" + "=" * 70 + "\n")

if __name__ == "__main__":
    main()
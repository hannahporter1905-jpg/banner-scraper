import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import sys
import os

def scrape_with_scraperapi_debug(url, api_key, country_code='us'):
    """
    Debug version - shows you exactly what ScraperAPI returns
    """
    
    api_url = 'http://api.scraperapi.com'
    
    params = {
        'api_key': api_key,
        'url': url,
        'country_code': country_code,
        'render': 'true',  # Enable JavaScript rendering
    }
    
    print(f"\nğŸŒ Sending request through ScraperAPI...")
    print(f"ğŸ“ Country: {country_code.upper()}")
    print(f"ğŸ”— Target: {url}")
    print(f"â³ Please wait (this may take 10-30 seconds)...\n")
    
    try:
        response = requests.get(api_url, params=params, timeout=90)
        response.raise_for_status()
        
        print("âœ… Successfully retrieved page!")
        print(f"ğŸ“Š Status Code: {response.status_code}")
        print(f"ğŸ“Š Page size: {len(response.content)} bytes")
        print(f"ğŸ“Š Content Type: {response.headers.get('Content-Type', 'unknown')}\n")
        
        # Save the HTML to a file so you can inspect it
        debug_filename = 'scraperapi_debug.html'
        with open(debug_filename, 'w', encoding='utf-8') as f:
            f.write(response.text)
        print(f"ğŸ’¾ Saved raw HTML to: {debug_filename}")
        print(f"   â†’ You can open this file in a browser to see what was retrieved\n")
        
        # Parse HTML
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Show page title
        title = soup.find('title')
        print(f"ğŸ“„ Page Title: {title.text if title else 'No title found'}\n")
        
        # Show first 500 characters of text
        page_text = soup.get_text()[:500]
        print(f"ğŸ“ First 500 characters of page text:")
        print(f"{'='*70}")
        print(page_text)
        print(f"{'='*70}\n")
        
        # Find ALL images (not just banners)
        all_images = soup.find_all('img')
        print(f"ğŸ–¼ï¸  Total <img> tags found: {len(all_images)}\n")
        
        if all_images:
            print("ğŸ“¸ All images found:")
            print(f"{'='*70}")
            for i, img in enumerate(all_images[:20], 1):  # Show first 20
                src = img.get('src', 'NO SRC')
                alt = img.get('alt', 'NO ALT')
                width = img.get('width', 'auto')
                height = img.get('height', 'auto')
                classes = img.get('class', [])
                
                print(f"\n  Image #{i}:")
                print(f"    SRC: {src[:100]}{'...' if len(src) > 100 else ''}")
                print(f"    ALT: {alt[:60]}{'...' if len(alt) > 60 else ''}")
                print(f"    SIZE: {width} x {height}")
                print(f"    CLASSES: {', '.join(classes) if classes else 'none'}")
            
            if len(all_images) > 20:
                print(f"\n  ... and {len(all_images) - 20} more images")
            print(f"{'='*70}\n")
        else:
            print("âŒ No <img> tags found on the page!\n")
            print("ğŸ” Checking for background images in CSS...")
        
        # Check for background images
        bg_elements = soup.find_all(style=lambda x: x and 'background-image' in x)
        print(f"ğŸ¨ Elements with background-image: {len(bg_elements)}\n")
        
        if bg_elements:
            print("Background images found:")
            print(f"{'='*70}")
            for i, elem in enumerate(bg_elements[:10], 1):
                style = elem.get('style', '')
                print(f"\n  BG Image #{i}:")
                print(f"    STYLE: {style[:100]}{'...' if len(style) > 100 else ''}")
                print(f"    TAG: <{elem.name}>")
                print(f"    CLASSES: {elem.get('class', [])}")
            print(f"{'='*70}\n")
        
        # Check for common banner containers
        print("ğŸ” Checking for common banner containers:")
        print(f"{'='*70}")
        banner_selectors = [
            ('header', soup.find_all('header')),
            ('.hero', soup.select('.hero')),
            ('.banner', soup.select('.banner')),
            ('.slider', soup.select('.slider')),
            ('.carousel', soup.select('.carousel')),
            ('#hero', soup.select('#hero')),
        ]
        
        for selector, elements in banner_selectors:
            if elements:
                print(f"  âœ… Found {len(elements)} element(s) matching '{selector}'")
                for elem in elements[:3]:
                    imgs_inside = elem.find_all('img')
                    print(f"      â†’ Contains {len(imgs_inside)} images")
            else:
                print(f"  âŒ No elements matching '{selector}'")
        
        print(f"{'='*70}\n")
        
        # Check if page looks like a redirect or error
        if len(response.text) < 5000:
            print("âš ï¸  WARNING: Page content is very small!")
            print("   This might be a redirect, login page, or error page.\n")
        
        # Look for common blocking messages
        blocking_keywords = ['captcha', 'access denied', 'forbidden', 'blocked', 'cloudflare', 'robot']
        found_blocks = []
        for keyword in blocking_keywords:
            if keyword in response.text.lower():
                found_blocks.append(keyword)
        
        if found_blocks:
            print(f"ğŸš« Possible blocking detected! Found keywords: {', '.join(found_blocks)}\n")
        
        return all_images
        
    except requests.RequestException as e:
        print(f"âŒ Error: {e}")
        return []

def main():
    """Interactive debug scraper"""
    
    print("\n" + "=" * 70)
    print("  ğŸ” ScraperAPI DEBUG Mode - See Exactly What Was Retrieved")
    print("=" * 70)
    print("\nThis version shows you:")
    print("  â€¢ The actual HTML content retrieved")
    print("  â€¢ All images found (not just banners)")
    print("  â€¢ Page structure and containers")
    print("  â€¢ Potential blocking issues")
    print("=" * 70 + "\n")
    
    # Get API key
    api_key = input("ğŸ”‘ Enter your ScraperAPI key: ").strip()
    
    if not api_key:
        print("\nâŒ API key is required!")
        sys.exit(1)
    
    # Get URL
    url = input("\nğŸŒ Enter website URL: ").strip()
    
    if not url:
        print("âŒ URL cannot be empty")
        sys.exit(1)
    
    if not url.startswith(('http://', 'https://')):
        url = 'https://' + url
    
    # Get country
    print("\nğŸ“ Available Countries: us, uk, ca, au, de, fr, jp, sg, br, in")
    country = input("ğŸ—ºï¸  Enter country code (default: us): ").strip().lower()
    if not country:
        country = 'us'
    
    print("\n" + "=" * 70)
    
    # Scrape and debug
    images = scrape_with_scraperapi_debug(url, api_key, country)
    
    print("=" * 70)
    print("  âœ… DEBUG COMPLETE")
    print("=" * 70)
    print(f"\nğŸ“ Check the file 'scraperapi_debug.html' to see the actual page")
    print(f"   Right-click the file â†’ Open with â†’ Your Browser\n")
    print("=" * 70 + "\n")

if __name__ == "__main__":
    main()